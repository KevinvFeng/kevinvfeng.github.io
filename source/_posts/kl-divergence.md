---
title: 深入理解KL Divergence
date: 2021-05-24 11:48:58
author: Kevin Feng
tags:
- Machine Learning
---
KL Divergence又叫KL散度，是VAE模型中的核心之一。
<!--more-->
## 背景知识
### 信息熵
信息熵在信息学中，体现了需要信息量的多少。举两个例子：
如果一个变量X 有50%的概率是0，50%的概率是1，则我们至少需要一个bit来确定它是0还是1。但如果这个变量X有100%的概率是0，我们就不需要任何额外的信息确定这个变量的值。
体现在公式上：
$H(x) = - \Sigma p(x)lg(p(x)) $
当X为第一种情况的时候（50%概率是0或1）
$H(x) = -\frac{1}{2}lg(\frac{1}{2}) * 2 = 1$
X为第二种情况的时候（100%是0）
$H(x) = -1*lg(1) = 0$
同理,在X可以等概率取0，1，2，3的时候，也就是每种概率都是25%的时候，$H(x) = 2$
这个信息熵可以体现在需要最少用多少的01编码表示这个变量。

|x|p(x)|p(x)编码|q(x)|q(x)最优编码|
|:----:|:----:|:----:|:----:|:-----:|
|0|0.25|00|0.5|0|
|1|0.25|01|0.25|10|
|2|0.25|10|0.125|110|
|3|0.25|11|0.125|111|
比如上面这个P(x) 每一中可能性的概率都是0.25，我们可以用00表示0，01表示1，10表示2，11表示3（也就是二进制啦）。在这种情况下，表示每一种情况都需要用两个bit进行编码。
而Q(X)，由于0，1，2，3的概率是不同的，我们可以改变编码方式
通过这种方式，我们降低了表示所有可能时编码的期望长度，现在只需要$0.5*1+0.25*2 + 0.125*3*2 = 1.75$个bit就可以表示变量X。这与信息熵公式是吻合的。
### 交叉熵
交叉熵包含“交叉”二字，所以意味着这需要两个分布，p以及q。交叉熵的实际意义是用p(x)的编码方式编码q(x)会需要多少信息。依然用上表举例，用p(x)编码q(x)：
$H(q,p) = 0.5*2+0.25*2+0.125*2 = 2$，即平均需要2bits表示q(x)，这大于最优方案1.75bit
那反过来用q(x)编码p(x)呢？
$H(p,q) = 0.25*1+0.25*2+0.25*3*2 = 2.25$ 同样比p(x)的熵 2要大。
## KL散度
回归正题，KL散度就是两个分布的熵的差异，也就是交叉熵减最优编码的熵
$KL(p||q) = H(p,q)-H(p)$
用上面的例子来说，就是 2.25-2 = 0.25
KL散度相较于交叉熵的优势在于，当两个分布相同的时候loss为0，而交叉熵并不